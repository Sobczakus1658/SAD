---
title: "Projekt SAD"
author: "Michał Sobczak"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Zadanie 1

**a)** Zacznijmy od wczytania danych.

```{r}
  X_train <- read.csv("~/SAD/data/X_train.csv")
  Y_train <- read.csv("~/SAD/data/Y_train.csv")
  X_test  <- read.csv("~/SAD/data/X_test.csv")
```

Sprawdźmy ile obserwacji i zmiennych zawierają wczytane dane treningowe oraz testowe

```{r}
  cat("Liczba obserwacji :", nrow(X_train), " liczba zmiennych: ", ncol(X_train), "\n")
  cat("Liczba obserwacji :", nrow(X_test),  " liczba zmiennych: ", ncol(X_test), "\n")
  cat("Liczba obserwacji :", nrow(Y_train), " liczba zmiennych: ", ncol(Y_train), "\n")
```

 W danych treningowych mamy podane 6800 obserwacji, w testowym - 1200. Każdy wiersz składa się z 9000 zmiennych objaśniających oraz 1 zmiennej objaśnianej. Sprawdźmy poprawność danych:
 
 ```{r}
   sum(is.na(X_train))
   sum(is.na(Y_train))
   sum(is.na(X_test))
 ```
 Widzimy, że wszystkie dane są kompletne. Zobaczmy jakiego są typu:
 
```{r}
  table(sapply(X_train, class))
  table(sapply(Y_train, class))
  table(sapply(X_test, class))
```

Widzimy, że są one wszystkie typu numerycznego. Możemy sprawdzić czy zawierają dane jakościowe. Ustalmy sobie pewną granicę - w moim projekcie będzie wynosić ona 3 - jeżeli w jednej kolumnie występują mniej niż 3 różne wartości to wtedy ta zmienna będzie jakościowa,  w pozostałym przypadku - ilościowa. Sprawdźmy różnorodność wartości na danych treningowych 

```{r}
X_variety <- min(sapply(X_train, function(col) length(unique(col))))
Y_variety <- min(sapply(Y_train, function(col) length(unique(col))))
X_variety
Y_variety
```

Widzimy, że każda kolumna przyjmuje co najmniej 12 różnych wartości, stąd wnioskujemy, że dane są ilościowe - nie jakościowe. Nie ma więc sensu konwertować je na typ factor.

**b)**
Zbadajmy rozkład empiryczny zmiennej objaśnianej. Przy okazji wyodrębnijmy Y na wektor, bo zawiera on tylko jedną zmienną

```{r}
summary(Y_train)
Y <-Y_train[[1]]
var(Y)
sd(Y)
```

Naszkicujmy teraz historam dla zmiennej objaśnianej. Na nim możemy zauważyć zaznaczony kolorem pomarańczony estymator gęstości.

```{r}
histogram <- hist(Y, probability = TRUE, main = "Histogram oraz estymowana gęstość CD36", col= "gray")
lines(density(Y), col = "orange", lwd = 2)  
```

**c)**

Wybierzmy 250 zmiennych objaśniających najbardziej skolerowanych ze zmienną objaśnianą

```{r}
library(reshape2)
corr <- sapply(X_train, function(col) cor(col, Y_train))
result <- X_train[, order(abs(corr), decreasing = TRUE)[1:250]]
corr_matrix = cor(result)
```

Wiemy, z wykładu że korelacja mierzy związek między dwiema zmiennymi. W przypadku dodatniego znaku mamy do czynienia z rosnącą zależnością liniową między zmiennymi, natomiast jeżeli korelacja jest ujemna to wtedy z malejącą zależnością liniową pomiędzy X i Y (antykorelacja). Zobaczmy, że bardziej interesuje nas moduł korelacji niż jej znak, dlatego od tego jeżeli będziemy wyznaczać najbardziej skorelowane zmienne to będziemy rozważać wartości z modułem.

Na końcu narysujmy heatmapę dla wybranych zmiennych. Nie możemy niestety używać podstawowej wersji heatmapy, gdyż nie można umieścić legendy obok niej. Dlatego zdecydowałem skorzystać z ggplot2, poniżej podaję [link](https://r-graph-gallery.com/79-levelplot-with-ggplot2.html) do strony, z której zaczerpnąłem wzorzec. Korelacja może przyjmwoać wartości od -1 do 1. By poprawić czytelność wykresu, gdyż mamy bardzo dużo zmiennych, usuniemy nazwy na osiach:

```{r}
library(ggplot2)

ggplot(melt(corr_matrix), aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "white", mid = "yellow", high = "red", 
                       midpoint = 0, limits = c(-1, 1), name = "Korelacja") +
  labs(title = "Macierz korelacji 250 zmiennych", x = "", y = "") +
  theme_minimal() +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank())
```

## Zadanie 2

**a)** Narysujmy wykres kwantylowy porównujący zmienną objaśnianą z rozkładem normalnym. Zaznaczmy również prostą wyznaczoną przez kwantyle doświadczalne. 

```{r}
qqnorm(Y, main = "Zmienna objaśniana a rozkład normalny")
qqline(Y, col = 'orange')
grid()
```

Możemy zauważyć, że dane znacząco odbiegają od rozkładu normalnego. Gdyby rozkład byłby zbliżony, to dane układałyby się wzdłuż linii.
Zgodnie z dokumentacją R, możemy przeczytać, że qqline powstaje przez poprowadzenie prostej przez pierwszy oraz trzeci kwartyl, łatwo więc odczytać nachylenie prostej oraz wyraz wolny. Gdyby dane pochodziły z rozkładu normalnego to nachylenie prostej byłoby równe odchyleniu standardowemu, a wyraz wolny byłby równy średniej. Obliczmy więc ile wynoszą te wartości na podstawie wykresu 

```{r}
q1 <- quantile(Y, c(0.25, 0.75))
q2 <- qnorm(c(0.25, 0.75))

slope <- (q1[1] - q1[2])/(q2[1] - q2[2])
intercept <- q1[1] - q2[1]
  
slope
intercept
```
Sprawdźmy ponadto ile rzeczywiście wynosi średnia oraz odchylenie standardowe dla naszych danych i czy wartości te są zbiżone do powyższych.

```{r}
mean(Y)
sd(Y)
```
Możemy zauważyć, że wyniki nie są zbliżone, stąd wnioskujemy, że na podstawie wykresu nie możemy odczytać średniej ani warinacji (bo wariancja to odchylenie standardowe podniesione do kwadratu, więc jeżeli odchylenie się różni, to wariancja również)

**b)**

Przeprowadźmy test statystyczny hipotezy zgodności zmiennej objaśnianej z rozkładem normalnym. W tym celu użyjemy dwustronnego testu [Kolmogorova-Smirnova](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test). Metoda ta jest wykorzystywana do testowania normalności danych. Zobaczmy, że nie możemy wykorzystać testu Shapiro-Wilka, ponieważ w R dla tego testu jest ustawiony limit wynoszący 5000 danych, a w naszym przypadku jest ich więcej.

Nasza hipoteza zerowa jest następującej postaci:
$$H_0:Y=N(\mu,\sigma^2)\text{ dla pewnych } \mu, \sigma^2 \quad H_1:Y\neq N(\mu, \sigma^2) \text{ dla wszystkich } {\mu}, \sigma^2,$$
Wykorzystamy do tego biblioteki nortest oraz funkcji lillie.test. W jej dokumentacji widzimy, że funkcja służy do sprawdzania hipotezy normalności danych. Ponadto sama estymuje najbardziej prawdopodobną średnią oraz wariancję. Ustalmy poziom istotności równy 0.01

```{r}
library(nortest)
summary <- lillie.test(Y)
print(summary)
```

Stąd odczytujemy, że p-wartość wynosi 2.2e-16 a to jest znacząco mniejsze od naszego poziomu istotności, więc możemy odrzucić naszą hipotezę zerową - Dane nie pochodzą z rozkładu normalnego.

**c)**

Analogicznie jak w 1c najpierw wybierzmy zmienną najbardziej skorelowaną ze zmienną objaśnianą

```{r}
index <- order(abs(corr), decreasing = TRUE)[1]
v <- X_train[[index]]
col_name <- colnames(X_train)[index]
col_name
```
Narysujmy najpierw jej histogram, żebyśmy mieli podstawową wiedzę o tym rozkładzie.

```{r}
hist(v, breaks = 50, probability = TRUE,  main = "Histogram RPL10", col= "gray")
```

Możemy zauważyć, że przypomina to rozkład Gamma, ale zbyt dużo wartości jest równych  zero. Możemy odrzucić hipotezę, że dane pochodzą z rozkładu dyskretnego, bo mamy za dużo różnych wartości. Możemy wysnuć również hipotezę, że urządzenie odczytu miało skończony próg odczytu i poniżej pewnej wartości urządzenie wskazywało zero. Uważam, że do rzetelniejszego sprawdzenia czy dane pochodzą z rozkładu Gamma, będzie dodanie do zerowych wartości lekkiego szumu, to znaczy wartości pochodzących z rozkładu normalnego $N(0,\epsilon)$. Do danych dodamy wartości bezwzględne, bo wszystkie dane są nieujemne oraz rozkład Gamma jest dla danych nieujemnych.

```{r}
epsilon <- 1e-3
v_mod <- v
v_mod[v_mod == 0] <- abs(rnorm(sum(v==0), 0, epsilon))
```

Zgodnie z [publikacją](https://alfa.im.pwr.edu.pl/~wilczyn/wstep%20do%20statystyki/wyklad%209.pdf) wiemy że MLE dla rozkładu Gamma istnieje, ale można je wyznaczyć jedynie numerycznie, dlatego użyjemy do tego wbudowanej biblioteki, zamiast pisać własne rozwiązanie numeryczne.

```{r}
library(MASS)
library(fitdistrplus)
fit_gamma <- fitdist(v_mod, "gamma", method = "mle")
shape_MLE = fit_gamma$estimate["shape"]
rate_MLE = fit_gamma$estimate["rate"]
```

Znowu użyjemy testu Kolmogorova-Smirnova. Ustalmy sobie poziom ufności równy 0.01. Nasz test jest postaci:
$$H_0:Y=\Gamma(shape_{MLE},rate_{MLE}) \quad H_1:Y\neq \Gamma(shape_{MLE}, rate_{MLE}) $$
```{r}
ks_result <- ks.test(v_mod, "pgamma", shape = shape_MLE, rate = rate_MLE)
print(ks_result)
```

Zobaczmy, że dodanie lekkiego szumu, sprawiło że dane poruszyły się lekko w prawo, co działa tylko na nasza korzyść. Znowu p-wartość wynosi bardzo mało i jest definitywnie mniejsza od naszego poziomu istotności, więc odrzucamy tę hipotezę zerową. Spójrzmy jeszcze na różnice między wykresem dyskrybuant rozkładów.

```{r}
plot(ecdf(v_mod), main = "Our Distribution vs Gamma CDF")
curve(pgamma(x, shape = shape_MLE, rate = rate_MLE), col = "orange", add = TRUE)
```

Wykres ten podtrzymuje nas przy zdaniu, ze odrzucenie hipotezy zerowej było słuszne.

Teraz sprawdzimy czy zmienna RPL10 ma podobny rozkład w zbiorze testowym oraz treningowym. Użyjemy do tego test Manna-Whitneya - jest to test do sprawdzenia, czy wartości z dwóch pobranych prób mają podobną wielkość. Weźmy X - rozkład zmiennej ze zbioru treningowego, Y - rozkład zmiennej ze zbioru testowego.

Ustalmy poziom istnotności równy = 0.01. Postawmy hipotezę zerową:
$$H_0: P(X>Y) = P(Y>X) \quad H_1: P(X>Y) \neq P(Y>X)$$
```{r}
Manna_result <- wilcox.test(v, X_test[["RPL10"]])
print(Manna_result)
```

Widzimy, że p wartość wyszła większa od poziomu istotności, więc nie mamy powodu by odrzucić hipotezę zerową.

## Zadanie 3

**a)**

ElasticNet to metoda regularyzacji regresji liniowej, która łączy elementy regresji Lasso (L1) oraz regresji grzbietowej (Ridge). Z natury działa ona lepiej niż samo stosowanie wyłącznie jednej opcji osobno. Kombinacja tych dwóch metod redukuje nieistotne współcznynniki do zera oraz unika nadmiernego dopasowania. Jej działanie kontrolują dwa parametry $\alpha$, która odpowiada za wagi przy karach oraz $\lambda$ odpowiadająca za siłę regularyzacji - jak mocno karzemy model.

W internecie możemy również spotkać wersję z dwoma parametrami $\alpha_1$ oraz $\alpha_2$, które odpowiadają za wagę przy regularyzacji Lasso oraz regularyzacji grzbietowej. My wybierzemy tę pierwszą wersję, dodatkowo dołączam link do strony z moją wersją [ElasticNet](https://medium.com/@abhishekjainindore24/elastic-net-regression-combined-features-of-l1-and-l2-regularization-6181a660c3a5).

Wzór na funkcję celu możemy zapisać jako:
$$ \text{Elastic Loss} = \frac{1}{2n}\sum_{i=1}^{n} (y_i - \hat{y_i})^2 + \lambda \left( \alpha \sum_{j=1}^p |\beta_j| + (1-\alpha)\sum_{j=1}^p \beta_j^2 \right) $$
gdzie n to liczba obserwacji (wierszy), natomiast p to liczba zmiennych (kolumn). W tym przypadku nieznane są współczynniki $\beta_1 \dots \beta_p$  i je będziemy estymować. Hiperparametrami są natomiast $\alpha$ oraz $\lambda$. W przypadku $\alpha = 1$ mamy do czynienia z LASSO, natomiast w przypadku kiedy $\alpha = 0$ - regresją grzbietową. 

**b)** Zdefiniujmy teraz siatkę hiperparamterów. W siatce wartości $\alpha$ umieścimy wartości 0 oraz 1 by znalazły się konfiguracje hiperparametrów odpowiadające regresji grzbietowej i lasso.

```{r}
alpha <- c(0, 0.50, 1) 
lambda <- c(0.1, 1, 5)
grid_elastic <- expand.grid(alpha = alpha, lambda = lambda)
n <- nrow(grid_elastic)
```

W dokumentacji funkcji cv.glmnet z pakietu glmnet możemy doczytać, że domyślną wartością zmiennej nfolds jest równa 10. Zauważmy jednak że w naszym przypadu mamy bardzo dużo danych i trenowanie z nfolds = 10 trwałoby bardzo długo dlatego w moim przypadku również użyjemy nfols = 7.

```{r}
library(caret)
n_folds = 7
folds <- createFolds(Y, k = n_folds, list = TRUE)
```

Zróbmy sobie teraz tablicę na wyniki dokładności naszych wyników.

```{r}
result <- matrix(NA, nrow = n, ncol = n_folds)
result_train <- matrix(NA, nrow = n, ncol = n_folds)
```

Musimy się przeiterować przez wszystkie wartości grid_elastic i dla każdej z możliwości wytrenować model oraz zapisać wyniki.

```{r}
library(glmnet)
library(Metrics)
for(i in 1:n) {
  break;
  alpha_i <- grid_elastic$alpha[i]
  lambda_i <- grid_elastic$lambda[i]
  tuneGrid <- data.frame(alpha = alpha_i, lambda = lambda_i)
    for (j in 1:n_folds) {
      id <- folds[[j]]
      X_train_cv <- X_train[-id, ]
      Y_train_cv <- Y[-id]
      X_val_cv <- X_train[id, ]
      Y_val_cv <- Y[id]
      
      model <- train(x = X_train_cv,
                  y = Y_train_cv, 
                  method='glmnet',
                  tuneGrid = tuneGrid,
                  trControl = trainControl(method = "none")
                )
        
      pred_val <- predict(model, newdata = X_val_cv)
      pred_train <- predict(model, newdata = X_train_cv)
        
      result[i,j] <- mse(Y_val_cv, pred_val)
      result_train[i,j] <- mse(Y_train_cv, pred_train)
    }
}

result <- read.csv("~/SAD/result/result.csv")
result_train  <- read.csv("~/SAD/result/result_train.csv")
```

Narysujmy teraz wykres skrzypcowy oraz zaznaczmy białymi kropkami pojedyncze wyniki MSE na danych walidacyjnych.

```{r}
df <- data.frame(
          alpha = rep(grid_elastic$alpha, each = n_folds),
          lambda = rep(grid_elastic$lambda, each = n_folds),
          fold = rep(1:n_folds, times = n),
          MSE = as.vector((t(result)))
      )
df$group <- paste0("(", df$alpha, ",", df$lambda, ")")

ggplot(df, aes(x = group, y = MSE, fill = group)) +
  geom_violin(width = 0.8, scale = "width",trim = FALSE, position = "identity")+
  geom_jitter(width = 0.2, size = 0.8, alpha = 0.5, color = "white") +
  theme_minimal(base_size = 16)+
    labs(
       title = "MSE dla różnych hiperparametrów",
       x = "Hiperparamtery (alpha, lambda)",
       y = "MSE",
       fill = "Parametry"
    )+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 16),
        legend.position = "right",
        )

```


**d)**

Wybierzemy błąd treningowy i walidacyjny dla hiperparametrów, dla których błąd średniokwadratowy walidacyjny jest najmniejszy. Najpierw policzymy dla jakich wartości $\alpha$ i $\lambda$ to najmniejsza wartość, a potem wybierzemy odpowiedni indeks z błędów treningowych.

```{r}
train_err_mean = apply(result_train, 1, mean)
val_err_mean = apply(result, 1, mean)
  
id_best_result_elastic <- which.min(val_err_mean)
val_err_mean[id_best_result_elastic]
```
Natomiast dla tych hiperparametrów błąd średniokwadratowy dla danych treningowych wynosi:

```{r}
train_err_mean[id_best_result_elastic]
```

## Zadanie 4

Na labie 12 została nam przedstawiona biblioteka randomForest - najpierw skorzystałem z tej biblioteki, lecz oczekiwanie na rezultaty trawało koło 7 godzin. Dlatego w dalszej części zdecydowałem się użyć biblioteki ranger. Używa ona mniej pamięci, działa znacząco szybciej niż randomForest oraz wspiera wielowątkowość. Wgłębiając się w jej dokumentację możemy zobaczyć, że trzy najbardziej sensowne hiperparametry to ntree, mtry oraz nodesize.

Ntree odpowiada ile drzew decyzyjnych ma zostać utworzonych, mtry za liczbę cech losowanych przy każdym podziale, natomiast nodesize to minimalny rozmiar węzła do podziału.

Będziemy postępować bardzo analogicznie jak poprzednio. Najpierw zainicjalizujmy niezbędne zmienne

```{r}
ntree <- c(50, 100, 150)
mtry <- c(100, 150, 200)
nodesize <- c(4, 5,6)
grid_forest <- expand.grid(ntree = ntree, mtry = mtry, nodesize = nodesize)
m <- nrow(grid_forest)
  
result_tree <- matrix(NA, nrow = m, ncol = n_folds)
result_train_tree <- matrix(NA, nrow = m, ncol = n_folds)
```

Teraz wykonajmy odpowiednią predykcję:

```{r}
library(ranger)
for(i in 1:m) {
  ntree_i <- grid_forest$ntree[i]
  mtry_i <- grid_forest$mtry[i]
  nodesize_i <- grid_forest$nodesize[i]
  for (j in 1:n_folds) {
    id <- folds[[j]]
    X_train_cv <- X_train[-id, ]
    Y_train_cv <- Y[-id]
    X_val_cv <- X_train[id, ]
    Y_val_cv <- Y[id]
    
    model <- ranger(
              Y_train_cv ~.,
              data = data.frame(Y_train_cv, X_train_cv),
              num.trees = ntree_i,
              mtry = mtry_i,
              min.node.size = nodesize_i,
            )
      
    pred_val <- predict(model, data = X_val_cv)$predictions
    pred_train <- predict(model, data = X_train_cv)$predictions
        
    result_tree[i,j] <- mse(Y_val_cv, pred_val)
    result_train_tree[i,j] <- mse(Y_train_cv, pred_train)
    break;
  }
break;
}

result_tree <- read.csv("~/SAD/result/result_tree.csv")
result_train_tree  <- read.csv("~/SAD/result/result_train_tree.csv")
```
**b)**
Narysujmy teraz wykres pudełkowy. 

```{r, fig.width = 14, fig.height=10}
library(scales)
df_tree <- data.frame(
    ntree = rep(grid_forest$ntree, each = n_folds),
    mtry = rep(grid_forest$mtry, each = n_folds),
    nodesize = rep(grid_forest$nodesize, each = n_folds),
    fold = rep(1:n_folds, times = m),
    MSE = as.vector((t(result_tree)))
)
df_tree$group <- paste0("(", df_tree$ntree, ",", df_tree$mtry, ",", df_tree$nodesize,")")

ggplot(data = df_tree, aes(x = group, y = MSE, fill = group)) +
  geom_boxplot() +
  theme_minimal(base_size = 16) + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 14),
    legend.position = "right",
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12),
    axis.title.x = element_text(size = 16, face = "bold", margin = ggplot2::margin(t = 30)),
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5, margin = ggplot2::margin(b = 20))
    )+
  xlab("Hiperparametry (ntree, mtry, nodesize)") +
  ylab("MSE") + 
  scale_fill_manual(values = hue_pal()(27), name = "Hiperparametry")  + 
  ggtitle("Wykres pudełkowy dla MSE dla konkretnych hiperparametrów")
```

**c)**
Postąpimy analogicznie jak w przypadku ElasticNeta, czyli znowu wybierzemy hiperparamtery dla których błąd walidacyjny był najmniejszy 

```{r}
val_err_mean_tree = apply(result_tree, 1, mean)
train_err_mean_tree = apply(result_train_tree, 1, mean)
  
id_best_result_forest <- which.min(val_err_mean_tree)
val_err_mean_tree[id_best_result_forest]
train_err_mean_tree[id_best_result_forest]
```

### Zadanie 5
Najpierw analogicznie jak wyżej obliczymy błędy dla modelu referencyjnego. Najpierw przygotujemy dane:

```{r}
result_refer <- rep(NA, n_folds)
result_train_refer <- rep(NA, n_folds)
```

A następnie obliczymy błędy:

```{r}
for (j in 1:n_folds) {
  id <- folds[[j]]
  X_train_cv <- X_train[-id, ]
  Y_train_cv <- Y[-id]
  X_val_cv <- X_train[id, ]
  Y_val_cv <- Y[id]
        
  pred <- mean(Y_train_cv)
  result_refer[j] <- mse(Y_val_cv, pred)
  result_train_refer[j] <- mse(Y_train_cv, pred)
}

val_err_mean_refer = mean(result_refer)
train_err_mean_refer = mean(result_train_refer)
  
val_err_mean_refer
train_err_mean_refer
```

Następnie przygotujmy dane do zaprezentowania w tabelce. Finalnie do zaprezentowania danych wykorzystamy bibliotekę Knitr.

```{r}
elasticTrain = as.numeric(result_train[id_best_result_elastic, ])
elasticVal = as.numeric(result[id_best_result_elastic, ])
forestTrain =  as.numeric(result_train_tree[id_best_result_forest, ])
forestVal = as.numeric(result_tree[id_best_result_forest, ])

final_result <- data.frame(
    Fold = paste0("Fold ", 1:7),
    ElasticNetTrainMSE = elasticTrain,
    ElasticNetValMSE = elasticVal,
    RandomForestTrainMSE = forestTrain,
    RandomForestValMSE = forestVal, 
    ReferTrainMSE = result_refer, 
    ReferValMSE = result_train_refer
  )

final_result <- rbind(
  final_result,
  data.frame(
    Fold = "średnia",
    ElasticNetTrainMSE = mean(elasticTrain),
    ElasticNetValMSE = mean(elasticVal),
    RandomForestTrainMSE = mean(forestTrain),
    RandomForestValMSE = mean(forestVal),
    ReferTrainMSE = train_err_mean_refer,
    ReferValMSE = val_err_mean_refer 
 )
)
```

Teraz wyświetlmy otrzymane wyniki 

```{r}
library(knitr)
kable(final_result, caption = "MSE modeli w walidacji krzyżowej")
```

Na podstawie powyższej tabelki, możemy wysnąć wniosek, że najmniejsze błędy średniokwadratowe są dla ElasticModel, stąd możemy nazwać go najlepszym modelem. Zobaczmy, że gdybyśmy dysponowali lepszym sprzętem to wyniki mogłyby się zmienić. W dokumentacji randomForest możemy doczytać, że preferowaną wartością parametru mtry, w przypadku regresji, jest  liczba zmiennych podzielona przez 3 - co w naszym przypadku wynosi 3000. Domyślna wartość parametru ntree wynosi 500, co również jest znacząco więcej niż liczby testowane przeze mnie. 